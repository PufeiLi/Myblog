---
layout: post
title: "深度学习基础理解-数学"
subtitle: "deep learning basic"
catalog: true
author: "WangW"
header-style: text
tags: 
    - Deep Learning
---

记录了本人关于《Deep Learning》的读书笔记，仅仅为了个人形象直观的理解，望周知;<!--break-->

目录：

[TOC]

## 线性代数相关

### 线性相关和生成子空间

如果逆矩阵$A^{-1}$存在，那么式子$Ax=b$对于每个向量都存在一个解。

为了分析方程有多少解，可以将矩阵$A$当作一个基（按列向量分）。那么就转化成有多少路径在基底$A$的水平下到达$b$；向量$x$中的元素应该表示沿着基地A的基向量应该走多少距离。

$$Ax=\sum_i x_i A_{:,i} \tag{1}​$$

式（1）这种操作一般被称为线性组合。在形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和。一组向量的**生成子空间**是原始向量线性组合后所能抵达的点的集合。

那么，确定$Ax=b$是否有解，相当于确定向量$b$是否在$A$列向量的生成子空间中。这个特殊的生成子空间被称为$A$的**列空间**或者$A$的**值域**。

### 范数

机器学习中，我们常使用**范数**来衡量一个向量的大小。形式上，$L^P​$范数的定义如下

$$||x||_p=(\sum_i |x_i|^p)^{\frac{1}{p}} \tag{2}​$$

更一般的，我们使用**Frobenius范数**来衡量矩阵的大小,类似与$L^2$范数，即

$$||A||_F=\sqrt{\sum_{i,j}A_{i,j}^2} \tag{3}$$

### 特征分解

关于特征分解的直观理解，我推荐看*马同学高等数学*微信公众号来理解；

在这其中，需要特别关注：

- 半正定矩阵  -  保证$x^TAx>=0$
- 正定矩阵 - 此外还保证$x^TAx=0 ==>x=0$

### 奇异值分解

和特征分解类似，我们将矩阵$A$分解成三个矩阵的乘积：

$$A=UDV^T \tag{4}$$

假设$A$是一个$m×n$的矩阵，那么$U$是一个$m×m$的矩阵，$D$是一个$ m×n$矩阵，$V$是$ n×n $矩阵；其中，$U, V$定义为正交矩阵，$D$定义为对角矩阵；

那么怎么解释奇异值分解呢？U的列向量为矩阵A的**左奇异向量**是$ AA^T $的特征向量；V的列向量是矩阵A的有奇异向量是$ A^TA $的特征向量；D的元素则是矩阵A的奇异值，是$ AA^T $特征值的平方根；

### Moore-Penrose 伪逆

定义出来伪逆主要是求解一般线性方程$ Ax=y ​$ 

计算伪逆的方法是基于一下公式的：

$$ A^+=VD^+U^T \tag{5}$$

最后，$ x = A^+y$

### 实例：主成分分析

手推公式



## 概率与信息论

### 条件概率

很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率。这种概率叫**条件概率**。可以通以下公式计算：

$$ P(y=y | x=x) = \frac{P(y=y, x=x)}{P(x=x)} \tag{6}$$

### 条件概率的链式法则

$$P(x^{(1)}, x^{(2)}, ..., x^{(n)}) = P(x^{(1)})\prod_{i=2}^n P(x^{(i)} | x^{(1)}, ..., x^{(i-1)}) \tag{7}$$

这个规则被称为概率的**链式法则**或者**乘法法则**。它可以从式（6）定义中直接获得。例如：

$$P(a, b, c) = P(a | b, c)P(b, c)$$

$$P(b,c)=P(b|c)P(c)$$

$$P(a,b,c) = P(a|b,c)P(b|c)P(c)$$

### 期望的表示方法

对于离散变量：

$$E_{x\sim P}[f(x)]=\sum_x {P(x)f(x)} \tag{8}$$

对于连续型变量：

$$E_{x\sim P}[f(x)]=\int{P(x)f(x)} dx \tag{9}​$$

**需要主要的是**当概率分布在上下文中明确指明时，可以简写为$ E_{x[f(x)]}$; 当期望作用的随机变量也很明确的时候可以完全不写$ E[f(x)]$.

### 常用的概率分布

- Bernoulli 分布 (伯努利分布)
- Multinoulli 分布
- 高斯分布
- 指数分布和Laplace分布
- Dirac分布和经验分布
- 混合的分布

### 信息论

#### 自信息

$$I(x)=-log P(x) \tag{10}$$

#### 香农熵

自信息只能处理单个的输出，我们用**香农熵**来对整个概率分布中的不确定性总量进行量化：

$$H(x)=E_{x\sim P}[I(x)] = -E_{x\sim P}[log P(x)] \tag{11}$$

也记作$H(P)$

#### KL散度

如果同一个随机变量x有连个单独的概率分布$P(x)$, $Q(x)$，可以使用**KL散度**来衡量这两个分布的差异：

$$D_{KL}(P || Q)=E_{x \sim P}[log \frac{P(x)}{Q(x)}] = E_{x \sim P}[log P(x)-log Q(x)] \tag{12}$$

#### 交叉熵

和KL散度密切相关的量时交叉熵，如下

$$H(P, Q) = H(P) + D_{KL}(P||Q) \tag{13}$$

## 数值计算

### 基于梯度的优化方法

在$u$（单位向量）方向的**方向导数**是函数$f$在$u$方向的斜率。可以直观的考虑到，当方向导数为正时，其方向指向函数的上坡，反之，指向下坡。因此想要求得下坡方向最**快**的方向，只需要求令方向导数最小的向量指向；则有定义可以知道$u$方向的方向导数为$f(x+au)$关于$a$的导数；于是$\frac{\partial }{\partial a}f(x+au)=u^T\bigtriangledown_xf(x)$

为了最小化$f$，需要找到使得$f$下降速度最快的方向，也就是是的$a$最小：

$$min_{u,u^Tu=1}u^T \bigtriangledown_xf(x) = min_{u,u^Tu=1}||u^T||_2 ||\bigtriangledown_xf(x)||_2 cos\theta$$

于是就得到了和方向导数方向相反的方向$u$.

梯度下降建议新的点为

$$x^‘ = x- \epsilon \bigtriangledown_xf(x)$$

### Jacobian 矩阵 和 Hessian 矩阵

有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数矩阵被称为**Jacobian**矩阵（雅可比矩阵）。具体来说，如果有一个函数$f: R^m \to R^n$, $f$的雅可比矩阵$J \in R^{n×m}$

我们有时也对导数的导数感兴趣，也就是二阶导数。当输入的函数具有多维输入时，二阶导数也有很多，将这些导数合并成一个矩阵，称为**Hessian**矩阵（海森矩阵）。直观理解曲率的作用，对于负曲率，函数是凸函数，那么代价函数根据梯度下降会比梯度预测下降的更快；反之，是慢的。

我们可以通过一个实例来看一下海森矩阵的作用。可以通过方向二阶导数预期一个梯度下降步骤能表现的有多好；我们在当前点$x^{(0)}$处做函数$f(x)$的近似二阶泰勒级数：

$$f(x)=f(x^{(0)})+(x-x^{(0)})^Tg + \frac{1}{2}(x-x^{(0)})^TH(x-x^{(0)})$$

那么通过梯度下降更新$x$点，新的$x$点将会是$x^{(0)}-\epsilon g$. 待入以上公式可得

$$f(x^{(0)}-\epsilon g)=f(x^{(0)})-\epsilon g^T g + \frac{1}{2}\epsilon ^2g^THg​$$

其中，第一项为原始值，第二项为函数斜率导致的期望改善，最后为函数的曲率导致的校正；可以看出，第三项控制实际的函数下降；

通过计算可以可得，使得近似泰勒级数下降最多的最优步长（学习率）为：

$$\epsilon ^*=\frac{g^Tg}{g^THg}$$

最坏的情况是$g$与$H$最大特征值$\lambda _{max}$对应的特征向量对齐，则最优步长为$\frac{1}{\lambda _{max}}$。

### 约束优化

参考:[约束优化方法](https://juejin.im/post/5b6b102ef265da0f783cc920)

### 实例：线性最小二乘

手推公式