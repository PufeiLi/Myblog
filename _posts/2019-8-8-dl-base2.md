---
layout: post
title: "深度学习基础理解-机器学习"
subtitle: "deep learning basic-2"
catalog: true
author: "WangW"
header-style: text
tags: 
    - Deep Learning
---

记录了本人关于《Deep Learning》的读书笔记，仅仅为了个人形象直观的理解，望周知;<!--break-->

目录：

[TOC]

## 学习算法

定义：对于 某类任务$T$和性能度量$P$，一个计算机程序被认为可以从经验$E$中学习是指，通过经验$E$改进后，它在任务$T$上由性能度量$P$衡量的性能有所提升。

### 任务 $T$

- 分类
- 输入缺失分类
- 回归
- 转录
- 机器翻译
- 结构化输出
- 异常检测
- 合成和采样
- 缺失值填补
- 密度估计或概率质量函数估计

### 性能度量 $P$

通常使用**准确率**, 或者**错误率**。 

### 经验 $E$

根据学习过程中的不同经验，机器学习算法可以大致分类为**无监督算法**和**监督算法**.

### 实例：线性回归

线性回归解决的是回归问题，换言之，目标是建立一个系统，将向量$x \in R^n$作为输入，预测标量$y \in R $作为输出。定义输出为

$$\hat{y}=w^Tx \tag{1}$$

其中$w \in R^n $是参数向量。

**任务$T$：** 通过输出$\hat{y} = w^Tx$,从$x$预测$y$. 

**度量$P$: 采用计算模型在测试集的均方误差**。如：$MSE_{test} = \frac{1}{m} \sum_i (\hat{y}^{(test)}-y^{(test)}) = \frac{1}{m} \|\|\hat{y}^{(test)}-y^{(test)}\|\|^2_2$

**经验 $E$:** 是指数据集中的分布，即在训练集中的*经验分布*.

在训练过程中，最小化$MSE_{train}$,可以简化为求解其导数为0的情况。

## 容量、过拟合和欠拟合

模型的容量是指其拟合各种函数的能力。

欠拟合是指模型不能在训练集上获得足够低的误差

过拟合是值训练集误差和测试集误差直接的差距过大

一种控制算法容量的方法是选择**假设空间**， 即学习算法可以选择为解决方案的函数集。

## 估计、偏差与方差

点估计等

### 偏差

定义：$ bias(\hat{\theta _m}) = E(\hat{\theta _m})-\theta$

**我们通过计算估计量的期望来决定它的偏差，也可以计算它的方差**

### 方差和标准差

方差告诉我们，当独立地从潜在的数据生成过程中重采样数据集时，如何期望估计的变化；

### 权衡偏差和方差以最小化均方误差

偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或者参数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差。

**结论的直观理解：**偏差大证明训练的函数偏离真实的函数；而方差大证明在测试集（相当于第二次采样）结果很大的可能是不理想，因为方差大导致估计期望的偏差就大。

判断权衡偏差和方差最常用的方法是**交叉验证**。另外，也可以比较这些估计的**均方误差（MSE）:**

$$MSE = E[(\hat{\theta _m}-\theta)^2] = Bias(\hat{\theta _m})^2 + Var(\hat{\theta _m})$$

### 一致性

希望当数据集中的数据点的数量$m$ 增加时，点估计会收敛到对应参数的真实值。更形式化的，我们想要：

$$plim_{m\to \infty}\hat{\theta _m} = \theta$$

## 最大似然估计

已经了解常用的估计的定义以及性质，但这些估计时从哪里来的呢？我们希望有一些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测，然后分析其偏差和方差。

最常用的准则就是**最大似然估计**

数据集$X=\{x^{(0)}, ..., x^{(m)}\}$,独立地有未知的真实数据生成分布$p_{data}(x)$生成。令$p_{model}(x;\theta)$将任意输入$x$映射到实数来估计真实概率$p_{data}(x)$.

对$\theta$的最大似然估计被定义为：

$$\theta _{ML}=argmax_\theta p_{model}(X; \theta) = argmax_\theta \prod_{i=1}^m p_{model}(x^{(i)}; \theta)$$

由于概率乘积不便于计算，为了得到等值优化问题，观察到似然对数不会改变 $arg max$ ,于是：

$$\theta_{ML} = argmax_\theta \sum_{i-1}^m logp_{model}(x^{(i)}; \theta)$$

除以$m$, 于是又得到了等值优化问题

$$\theta_{ML}=argmax_\theta E_{x\sim \hat{p}_{data}}[log p_{model}(x; \theta)]$$     **交叉熵**

### 条件对数似然和军方误差

最大似然估计很容易扩展到估计条件概率；对数似然也就扩展到了条件对数似然；

**示例：线性回归作为最大似然** 

拟合分布$p(y\|x)$, 定义$p(y\|x) = N(y; \hat{y}(x; w), \delta^2)$ **???**

## 贝叶斯统计

参考：

## 随机梯度下降

## 构建机器学习算法

几乎所有的深度学习算法都可以被描述为一个相当简单的配方：

- 特定的数据集
- 代价函数
- 优化过程
- 模型

例如：线性回归算法由一下部分组成：$X$ 和 $y$ 构成数据集；代价函数为：

$$J(w, b) = -E_{x,y \sim \hat{p}_{data}}log p_{model}(y|x)$$

模型是$p_{model}(y \| x) = N(y; x^Tw + b, 1)$, 大多数情况下，优化算法可以定义为求解代价函数梯度为零的正规方差；