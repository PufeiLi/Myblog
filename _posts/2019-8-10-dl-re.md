---
layout: post
title: "深度学习之正则化"
subtitle: ""
catalog: true
author: "WangW"
header-style: text
tags: 
    - Deep Learning
---

记录了本人关于《Deep Learning》的读书笔记，仅仅为了个人形象直观的理解，望周知;<!--break-->

目录：

[TOC]

将正则化定义为“对学校算法的修改”---旨在减少泛化误差而不是训练误差。

## 参数范数惩罚

### $L^2$参数正则化

该权重的衰减效果就是沿着由海森矩阵的特征向量所定义的轴缩放$w^*$. 具体来说，会根据$\frac{\lambda _i}{\lambda_i + a}$因子缩放与海森矩阵第$i$个特征向量对齐的$w^*$的分量。**推导过程包含假设未正则化 $ w^* $ ，以及二次近似（泰勒公式展开）**

沿着H特征值较大的方向，正则化的影响较小。而特征值很小的方向，将会收缩几乎为0

### $L^1$正则化

$L^1$正则化会产生更**稀疏**的解。被广泛应用与特征选择（feature selection）

## 作为约束的范数惩罚

## 数据集增强

## 噪声鲁棒性

对于某些模型来说，向输入添加方差极小的噪声等价于对权重施加范数惩罚。

另一种正则化模型的噪声使用方式时将其加到权重。主要用于RNN。这项技术可以被解释为关于权重的贝叶斯推断的随机实现。

### 向输出目标注入噪声

大多数数据集的$y$标签有一点的错误。

## 半监督学习

## 多任务学习

## 提前终止

提前终止为何具有正则化效果，**推导**

提前终止比权重衰减更具有优势，提前终止能自动确定正则化的正确量，而权重衰减需要进行多个不同超参数值的训练实验。

## 参数绑定和参数共享

### 卷积神经网络

### 使训练的模型参数逼近老师模型

## 稀疏表示

前面所讲的权重衰减直接惩罚模型参数。另一种策略时惩罚神经网络中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚

## Bagging 和其它集成方法

通过所有集成模型的平均预测所得误差是 $\frac{1}{k} \sum _i \epsilon _i​$ 。集成预测器平方误差的期望是：

$$E[(\frac{1}{k} \sum _i \epsilon _i)^2]=\frac{1}{k^2} E[\sum _i (\epsilon _i^2 + \sum _{j\not =i}\epsilon _i \epsilon _j)]=\frac{1}{k}v + \frac{k-1}{k}c$$

看的出来，在误差完全相关即$c=v$的情况下，均方误差减少到$v$，模型没有任何改进。在模型完全不相关的情况下即$c=0$, 该集成平方误差的期望仅仅为$\frac{1}{k}v$。

### 其它集成方法

**如boosting技术**

## Dropout

参考

## 对抗训练

## 切面距离、正切传播和流形正切分类器

